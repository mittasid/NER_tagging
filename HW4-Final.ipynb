{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headers = [\"index\", \"word\", \"ner_tag\"]\n",
    "train_df = pd.read_csv(\"./data/train\", sep=' ', header=None, quoting=3)\n",
    "train_df.columns = train_headers\n",
    "\n",
    "dev_df = pd.read_csv(\"./data/dev\", sep=' ', header=None, quoting=3)\n",
    "dev_df.columns = train_headers\n",
    "\n",
    "test_headers = [\"index\", \"word\"]\n",
    "test_df = pd.read_csv(\"./data/test\", sep=' ', header=None, engine='python', error_bad_lines=False, quoting=3)\n",
    "test_df.columns = test_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"word\"] = train_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)\n",
    "dev_df[\"word\"] = dev_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)\n",
    "test_df[\"word\"] = test_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = {}\n",
    "for row in train_df.iterrows():\n",
    "    if row[1][\"word\"] in word_frequency:\n",
    "        word_frequency[row[1][\"word\"]] += 1\n",
    "    else:\n",
    "        word_frequency[row[1][\"word\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "unknown_words = [] \n",
    "for key, value in word_frequency.items():\n",
    "    if value < threshold:\n",
    "        unknown_words.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"word\"] = train_df[\"word\"].apply(lambda x: \"<unk>\" if x in unknown_words else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"word\"] = train_df[\"word\"].astype(str)\n",
    "train_vocab_size = len(np.unique(train_df[\"word\"]))\n",
    "all_train_words = np.unique(train_df[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df[\"word\"] = dev_df[\"word\"].apply(lambda x: \"<unk>\" if x in unknown_words else (\"<unk>\" if x not in all_train_words else x))\n",
    "dev_df[\"word\"] = dev_df[\"word\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(df):\n",
    "    train_formatted = []\n",
    "    first = df.iloc[0]\n",
    "    x = [first[\"word\"]]\n",
    "    y = [first[\"ner_tag\"]]\n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            train_formatted.append([x, y])\n",
    "            x, y = [], []\n",
    "            x.append(row[1][\"word\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([x, y])\n",
    "        else:\n",
    "            x.append(row[1][\"word\"])\n",
    "            y.append(row[1][\"ner_tag\"])\n",
    "    return train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted = format_data(train_df)\n",
    "dev_formatted = format_data(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map = {\"<pad>\":0}\n",
    "for i, word in enumerate(set(train_df[\"word\"])):\n",
    "    word_map[word] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = {\"<pad>\":-1}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    tag_map[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map_without_pad = {}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    tag_map_without_pad[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sentence = max([len(s[0]) for s in train_formatted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_formatted):\n",
    "    train_padded = []\n",
    "    for sentence in sentences_formatted:\n",
    "        word_lst = sentence[0]\n",
    "        ner_lst = sentence[1]\n",
    "        mapped_word_lst, mapped_ner_lst = [], []\n",
    "        for word in word_lst:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "        for ner in ner_lst:\n",
    "            mapped_ner_lst.append(tag_map[ner])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = max_len_sentence - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        mapped_ner_lst = mapped_ner_lst + [-1] * diff_\n",
    "\n",
    "        train_padded.append([mapped_word_lst, mapped_ner_lst])\n",
    "    return train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sentences(train_formatted)\n",
    "dev_padded = pad_sentences(dev_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Simple Bidirectional LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        linear_out_dropped = self.dropout(out)\n",
    "        elu_out = self.elu(linear_out_dropped)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.1\n",
      "Epoch: 1 \tTraining Loss: 0.013217 \tTest Loss: 0.012173\n",
      "Epoch-1 lr: 0.1\n",
      "Epoch: 2 \tTraining Loss: 0.011612 \tTest Loss: 0.011400\n",
      "Epoch-2 lr: 0.1\n",
      "Epoch: 3 \tTraining Loss: 0.010564 \tTest Loss: 0.010077\n",
      "Epoch-3 lr: 0.1\n",
      "Epoch: 4 \tTraining Loss: 0.009544 \tTest Loss: 0.009327\n",
      "Epoch-4 lr: 0.1\n",
      "Epoch: 5 \tTraining Loss: 0.008793 \tTest Loss: 0.008687\n",
      "Epoch-5 lr: 0.1\n",
      "Epoch: 6 \tTraining Loss: 0.008140 \tTest Loss: 0.008190\n",
      "Epoch-6 lr: 0.1\n",
      "Epoch: 7 \tTraining Loss: 0.007589 \tTest Loss: 0.007709\n",
      "Epoch-7 lr: 0.1\n",
      "Epoch: 8 \tTraining Loss: 0.007064 \tTest Loss: 0.007310\n",
      "Epoch-8 lr: 0.1\n",
      "Epoch: 9 \tTraining Loss: 0.006607 \tTest Loss: 0.007015\n",
      "Epoch-9 lr: 0.05\n",
      "Epoch: 10 \tTraining Loss: 0.005940 \tTest Loss: 0.006579\n",
      "Epoch-10 lr: 0.05\n",
      "Epoch: 11 \tTraining Loss: 0.005729 \tTest Loss: 0.006494\n",
      "Epoch-11 lr: 0.05\n",
      "Epoch: 12 \tTraining Loss: 0.005554 \tTest Loss: 0.006363\n",
      "Epoch-12 lr: 0.05\n",
      "Epoch: 13 \tTraining Loss: 0.005381 \tTest Loss: 0.006306\n",
      "Epoch-13 lr: 0.05\n",
      "Epoch: 14 \tTraining Loss: 0.005244 \tTest Loss: 0.006202\n",
      "Epoch-14 lr: 0.05\n",
      "Epoch: 15 \tTraining Loss: 0.005095 \tTest Loss: 0.006081\n",
      "Epoch-15 lr: 0.05\n",
      "Epoch: 16 \tTraining Loss: 0.004973 \tTest Loss: 0.005957\n",
      "Epoch-16 lr: 0.05\n",
      "Epoch: 17 \tTraining Loss: 0.004859 \tTest Loss: 0.005849\n",
      "Epoch-17 lr: 0.05\n",
      "Epoch: 18 \tTraining Loss: 0.004740 \tTest Loss: 0.005855\n",
      "Epoch-18 lr: 0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-55550c2c6056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-71edbb71b4f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mlstm_out_dropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out_dropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 680\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EMBEDDING_DIM = 100\n",
    "# VOCAB_SIZE = train_vocab_size+1\n",
    "# n_epochs = 20\n",
    "# trainloader = torch.utils.data.DataLoader(train_padded, batch_size=50)\n",
    "# devloader = torch.utils.data.DataLoader(dev_padded, batch_size=50)\n",
    "# blstm = BLSTM(VOCAB_SIZE)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True)\n",
    "# optimizer = torch.optim.SGD(blstm.parameters(), lr=0.1, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# test_loss_min = 10000\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     scheduler.step()\n",
    "#     print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "#     train_loss = 0\n",
    "#     test_loss = 0\n",
    "#     blstm.train()\n",
    "#     for data, target in trainloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         output = blstm(torch.cat(data,dim=0))\n",
    "#         loss = criterion(output, torch.cat(target,dim=0))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in devloader:\n",
    "#             output = blstm(torch.cat(data,dim=0))\n",
    "#             loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss\n",
    "#     train_loss = train_loss/len(trainloader.dataset)\n",
    "#     test_loss = test_loss/len(devloader.dataset)\n",
    "#     print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "#         epoch+1, \n",
    "#         train_loss,\n",
    "#         test_loss\n",
    "#         ))    \n",
    "#     if test_loss <= test_loss_min:\n",
    "#         torch.save(blstm.state_dict(), 'blstm1.pt')\n",
    "#         test_loss_min = test_loss\n",
    "# print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_test_sentences(sentences_formatted):\n",
    "    test_padded = []\n",
    "    for sentence in sentences_formatted:\n",
    "        mapped_word_lst = []\n",
    "        for word in sentence:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = max_len_sentence - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "\n",
    "        test_padded.append(mapped_word_lst)\n",
    "    return test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_test(df):\n",
    "    test_formatted = []\n",
    "    first = df.iloc[0]\n",
    "    x = [first[\"word_formatted\"]]\n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            test_formatted.append(x)\n",
    "            x = []\n",
    "            x.append(row[1][\"word_formatted\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([x])\n",
    "        else:\n",
    "            x.append(row[1][\"word_formatted\"])\n",
    "    \n",
    "    return test_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"word_formatted\"] = test_df[\"word\"].apply(lambda x: \"<unk>\" if x in unknown_words else (\"<unk>\" if x not in all_train_words else x))\n",
    "test_df[\"word_formatted\"] = test_df[\"word_formatted\"].astype(str)\n",
    "\n",
    "test_formatted = format_data_test(test_df)\n",
    "test_padded = pad_test_sentences(test_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm.load_state_dict(torch.load('blstm1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, dataloader, data_test):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data_test):\n",
    "        non_padded_pred = len(np.nonzero(sentence)[0])\n",
    "        pred_i = prediction_list[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    convert_overall_pred = []\n",
    "    for sentence in overall_pred:\n",
    "        for idx in sentence:\n",
    "            convert_overall_pred.append(list(tag_map_without_pad.keys())[idx])\n",
    "    return convert_overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dev(model, dataloader, data_dev):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data_dev):\n",
    "        actual_sentence = sentence[0]\n",
    "        non_padded_pred = len(np.nonzero(actual_sentence)[0])\n",
    "        pred_i = prediction_list[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    convert_overall_pred = []\n",
    "    for sentence in overall_pred:\n",
    "        for idx in sentence:\n",
    "            convert_overall_pred.append(list(tag_map_without_pad.keys())[idx])\n",
    "    return convert_overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=1)\n",
    "predictions_dev = predict_dev(blstm, devloader, dev_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(dev_df[\"ner_tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(name, y_true, y_pred, df):\n",
    "    with open(name, 'w') as f:\n",
    "        for row in df.iloc[0:].iterrows():\n",
    "            f.write(str(row[1][\"index\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(row[1][\"word\"])\n",
    "            f.write(\" \")\n",
    "            f.write(y_true[row[0]])\n",
    "            f.write(\" \")\n",
    "            f.write(y_pred[row[0]])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_test(name, y_pred, df):\n",
    "    with open(name, 'w') as f:\n",
    "        for row in df.iloc[0:].iterrows():\n",
    "            f.write(str(row[1][\"index\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(row[1][\"word\"])\n",
    "            f.write(\" \")\n",
    "            f.write(y_pred[row[0]])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev1.out\", y_true, predictions_dev, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(test_padded, batch_size=1)\n",
    "predictions_test = predict_test(blstm, testloader, test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-037d8204d9b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_results_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test1.out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-c1c84ffdc1e4>\u001b[0m in \u001b[0;36mwrite_results_test\u001b[0;34m(name, y_pred, df)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "write_results_test(\"test1.out\", predictions_test, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Using GloVe Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./glove.6B.100d\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    word2vec={}\n",
    "    for word_embedding in f:\n",
    "        word_split = word_embedding.split()\n",
    "        word = word_split[0]\n",
    "        word2vec[word] = np.array(word_split[1:], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map_2 = {}\n",
    "for i, word in enumerate(set(train_df[\"word\"]).union(set(dev_df[\"word\"]))):\n",
    "    word_map_2[word] = i+1\n",
    "word_map_2[\"<unk>\"] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 101\n",
    "VOCAB_SIZE = len(word_map_2)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, idx in word_map.items():\n",
    "    if word in word2vec:\n",
    "        word_embedding = word2vec[word]\n",
    "        embedding_matrix[idx,:] = np.concatenate((word_embedding, [0]))\n",
    "    elif word.lower() in word2vec: \n",
    "        word_embedding = word2vec[word.lower()]\n",
    "        embedding_matrix[idx,:] = np.concatenate((word_embedding, [1]))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "embedding_blstm2 = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM) \n",
    "embedding_blstm2.load_state_dict({\"weight\": torch.tensor(embedding_matrix)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_glove(df):\n",
    "    train_formatted = []\n",
    "    first = df.iloc[0]\n",
    "    x = [first[\"word\"]]\n",
    "    y = [first[\"ner_tag\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            train_formatted.append([x, y])\n",
    "            x, y = [], []\n",
    "            x.append(row[1][\"word\"])\n",
    "            y.append(row[1][\"ner_tag\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([x, y])\n",
    "        else:\n",
    "            x.append(row[1][\"word\"])\n",
    "            y.append(row[1][\"ner_tag\"])\n",
    "    return train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted_glove = format_data_glove(train_df)\n",
    "dev_formatted_glove = format_data_glove(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_test_glove(df):\n",
    "    test_formatted = []\n",
    "    first = df.iloc[0]\n",
    "    x = [first_word[\"word_formatted\"]]\n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            test_formatted.append(x)\n",
    "            x = []\n",
    "            x.append(row[1][\"word_formatted\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                test_formatted.append(x)\n",
    "        else:\n",
    "            x.append(row[1][\"word_formatted\"])\n",
    "    return test_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_glove(sentences_formatted):\n",
    "    train_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        word_lst = sentence[0]\n",
    "        ner_lst = sentence[1]\n",
    "        mapped_word_lst, mapped_ner_lst = [], []\n",
    "        cnt += len(word_lst)\n",
    "        for word in word_lst:\n",
    "            mapped_word_lst.append(word_map_2[word])\n",
    "        for ner in ner_lst:\n",
    "            mapped_ner_lst.append(ner_map[ner])\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        mapped_ner_lst = mapped_ner_lst + [-1] * diff_\n",
    "        train_padded.append([mapped_word_lst, mapped_ner_lst])\n",
    "    return train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_glove = pad_glove(train_formatted_glove)\n",
    "dev_padded_glove = pad_glove(dev_formatted_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_test_glove(sentences_formatted):\n",
    "    test_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        mapped_word_lst = []\n",
    "        cnt += len(sentence)\n",
    "        for word in sentence:\n",
    "            mapped_word_lst.append(word_map_2[word])\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        test_padded.append(mapped_word_lst)\n",
    "    return test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"word_formatted\"] = test_df[\"word\"].apply(lambda x: \"<unk>\" if x not in word_map_2 else x)\n",
    "test_df[\"word_formatted\"] = test_df[\"word_formatted\"].astype(str)\n",
    "test_formatted_glove = format_data_test(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded_glove = pad_test_sentences_glove(test_formatted_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(input_size=101, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.elu = nn.ELU()\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        elu_out = self.elu(out)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.1\n",
      "Epoch: 1 \tTraining Loss: 0.020680 \tTest Loss: 0.018366\n",
      "Epoch-1 lr: 0.1\n",
      "Epoch: 2 \tTraining Loss: 0.014163 \tTest Loss: 0.018013\n",
      "Epoch-2 lr: 0.1\n",
      "Epoch: 3 \tTraining Loss: 0.013037 \tTest Loss: 0.017807\n",
      "Epoch-3 lr: 0.1\n",
      "Epoch: 4 \tTraining Loss: 0.012327 \tTest Loss: 0.017678\n",
      "Epoch-4 lr: 0.1\n",
      "Epoch: 5 \tTraining Loss: 0.011875 \tTest Loss: 0.017604\n",
      "Epoch-5 lr: 0.1\n",
      "Epoch: 6 \tTraining Loss: 0.011515 \tTest Loss: 0.017511\n",
      "Epoch-6 lr: 0.1\n",
      "Epoch: 7 \tTraining Loss: 0.011281 \tTest Loss: 0.017460\n",
      "Epoch-7 lr: 0.1\n",
      "Epoch: 8 \tTraining Loss: 0.011062 \tTest Loss: 0.017408\n",
      "Epoch-8 lr: 0.1\n"
     ]
    }
   ],
   "source": [
    "# EMBEDDING_DIM = 101\n",
    "# VOCAB_SIZE = len(word_map_2)\n",
    "# n_epochs = 20\n",
    "# trainloader = torch.utils.data.DataLoader(train_padded_glove, batch_size=16)\n",
    "# devloader = torch.utils.data.DataLoader(dev_padded_glove, batch_size=16)\n",
    "# blstm2 = BLSTM_2(embedding_blstm2)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True)\n",
    "# optimizer = torch.optim.SGD(blstm2.parameters(), lr=0.1, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# test_loss_min = 10000\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     scheduler.step()\n",
    "#     print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "#     train_loss = 0\n",
    "#     test_loss = 0\n",
    "#     blstm2.train()\n",
    "#     for data, target in trainloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         output = blstm2(torch.cat(data,dim=0))\n",
    "#         loss = criterion(output, torch.cat(target,dim=0))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss \n",
    "#     with torch.no_grad():\n",
    "#         for data, target in devloader:\n",
    "#             output = blstm2(torch.cat(data,dim=0))\n",
    "#             loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss\n",
    "#     train_loss = train_loss/len(trainloader.dataset)\n",
    "#     test_loss = test_loss/len(devloader.dataset) \n",
    "#     print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "#         epoch+1, \n",
    "#         train_loss,\n",
    "#         test_loss\n",
    "#         ))   \n",
    "#     if test_loss <= test_loss_min:\n",
    "#         torch.save(blstm2.state_dict(), 'blstm2.pt')\n",
    "#         test_loss_min = test_loss\n",
    "# print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm2 = BLSTM_2(embedding_blstm2)\n",
    "blstm2.load_state_dict(torch.load('blstm2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "devloader = torch.utils.data.DataLoader(dev_padded_glove, batch_size=1)\n",
    "predictions_dev_glove = predict_dev(blstm2, devloader, dev_padded_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(dev_df[\"ner_tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev2.out\", y_true, predictions_dev_glove, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(test_padded_glove, batch_size=1)\n",
    "predictions_test_glove = predict_test(blstm2, testloader, test_padded_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-cc3173dfec61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_results_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test2.out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_test_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-c1c84ffdc1e4>\u001b[0m in \u001b[0;36mwrite_results_test\u001b[0;34m(name, y_pred, df)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "write_results_test(\"test2.out\", predictions_test_glove, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
